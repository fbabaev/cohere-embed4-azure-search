{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452cc653",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6358eff9",
   "metadata": {},
   "source": [
    "# Using Cohere Embed-4 Embedding Model with Azure AI Search for Multimodal Search\n",
    "\n",
    "This notebook shows how to use Cohere Embed-4 on Azure to generate text and image embeddings and index them in Azure AI Search for multimodal vector search across documents and images.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* **Azure Account**: Ensure you have an active Azure account with access to Azure AI services and Azure Cognitive Search.\n",
    "* **Cohere Embed-4 Model**: Deploy the Cohere Embed-4 model in Azure AI Foundry.\n",
    "* **API Keys and Endpoints**: Obtain the necessary API keys and endpoints for Azure AI and Azure Cognitive Search.\n",
    "* **Sample Data**: Have sample images available for embedding.\n",
    "\n",
    "## Install Required Packages\n",
    "\n",
    "First, install the necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf63a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-search-documents in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (11.6.0b12)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (1.35.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (4.14.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-search-documents python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d0744",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217225b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-search-documents in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (11.6.0b12)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (1.35.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-search-documents) (4.14.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\t-fbabaev\\appdata\\local\\programs\\python\\python313-arm64\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e383d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d33899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureMachineLearningParameters,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e430c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a244105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure AI Studio Cohere Configuration\n",
    "AZURE_AI_STUDIO_COHERE_EMBED_KEY = os.getenv(\"AZURE_AI_STUDIO_COHERE_EMBED_KEY\")\n",
    "AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT = os.getenv(\"AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT\")\n",
    "\n",
    "# Azure Cognitive Search Configuration\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "AZURE_SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = \"multimodal-cohere-embed4-index\"\n",
    "\n",
    "# Cohere Model Information\n",
    "EMBEDDING_MODEL_NAME = \"embed-4\"\n",
    "EMBEDDING_DIMENSIONS = 1536  # Embed-4 uses 1536 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9370a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All required environment variables are set\n"
     ]
    }
   ],
   "source": [
    "# Validate required environment variables\n",
    "required_vars = {\n",
    "    \"AZURE_AI_STUDIO_COHERE_EMBED_KEY\": AZURE_AI_STUDIO_COHERE_EMBED_KEY,\n",
    "    \"AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT\": AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT,\n",
    "    \"AZURE_SEARCH_SERVICE_ENDPOINT\": AZURE_SEARCH_SERVICE_ENDPOINT,\n",
    "    \"AZURE_SEARCH_ADMIN_KEY\": AZURE_SEARCH_ADMIN_KEY\n",
    "}\n",
    "\n",
    "missing_vars = [name for name, value in required_vars.items() if not value]\n",
    "if missing_vars:\n",
    "    print(\"❌ Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"  - {var}\")\n",
    "    print(\"\\nPlease set these environment variables before proceeding.\")\n",
    "else:\n",
    "    print(\"✅ All required environment variables are set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f845203",
   "metadata": {},
   "source": [
    "## Authenticate Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faaba864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate Azure Cognitive Search client\n",
    "azure_search_credential = AzureKeyCredential(AZURE_SEARCH_ADMIN_KEY)\n",
    "\n",
    "# Initialize Azure Search clients\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=AZURE_SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=azure_search_credential\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_SERVICE_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=azure_search_credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaab6a3",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "### Text Documents and Image URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7821b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in dataset: 4\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"caption\": \"A man wearing a beanie and shirt works on a laptop, representing modern remote work and technology usage\",\n",
    "        \"imageUrl\": \"https://images.unsplash.com/photo-1755541516517-bb95790dc7ad\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"caption\": \"A baby elephant sits on a rock surrounded by adult elephants in their natural habitat, showcasing wildlife family dynamics and conservation\",\n",
    "        \"imageUrl\": \"https://images.unsplash.com/photo-1756767265856-c0e726dd9896\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"caption\": \"A man photographs two girls sitting in an open car trunk, capturing a casual outdoor photography session with friends\",\n",
    "        \"imageUrl\": \"https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"caption\": \"A hockey player takes aim for the goal while facing the goalie on an ice rink, capturing the intensity and precision of competitive ice hockey\",\n",
    "        \"imageUrl\": \"https://images.unsplash.com/photo-1516226415502-d6624544376b\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Total documents in dataset: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278f9e2",
   "metadata": {},
   "source": [
    "## Functions to Encode Images and Generate Embeddings\n",
    "\n",
    "### Encode Image to Base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72f59f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert image URL to base64 data URI format required by Cohere Embed-4.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers['Content-Type']\n",
    "        base64_data = base64.b64encode(response.content).decode('utf-8')\n",
    "        return f\"data:{content_type};base64,{base64_data}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image {image_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50326664",
   "metadata": {},
   "source": [
    "### Generate Text Embeddings using REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb07a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def generate_text_embedding(text: str, model=EMBEDDING_MODEL_NAME) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate text embeddings using Cohere Embed-4 via direct REST API calls.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to embed.\n",
    "        model (str): The embedding model name.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {AZURE_AI_STUDIO_COHERE_EMBED_KEY}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"texts\": [text],\n",
    "        \"input_type\": \"search_document\",\n",
    "        \"truncate\": \"NONE\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT}/v1/embed\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"embeddings\"][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text embedding: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069538a",
   "metadata": {},
   "source": [
    "### Generate Image Embeddings using REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a051ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def generate_image_embedding(image_base64: str, model=EMBEDDING_MODEL_NAME) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate image embeddings using Cohere Embed-4 via direct REST API calls.\n",
    "    \n",
    "    Args:\n",
    "        image_base64 (str): The base64 encoded image.\n",
    "        model (str): The embedding model name.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {AZURE_AI_STUDIO_COHERE_EMBED_KEY}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"images\": [image_base64],\n",
    "        \"input_type\": \"image\",\n",
    "        \"truncate\": \"NONE\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{AZURE_AI_STUDIO_COHERE_EMBED_ENDPOINT}/v1/embed\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"embeddings\"][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image embedding: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf3ce9",
   "metadata": {},
   "source": [
    "## Process and Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing with extended delays due to rate limiting...\n",
      "This may take several minutes...\n",
      "\n",
      "Processing document 1/4: 1\n",
      "  Waiting 30 seconds before text embedding...\n",
      "  Generating text embedding...\n",
      "  ✓ Text embedding generated\n",
      "  Waiting 45 seconds before image processing...\n",
      "  Encoding image...\n",
      "  Generating image embedding...\n",
      "  ✓ Image embedding generated\n",
      "  ✓ Document 1 processed successfully\n",
      "  Waiting 60 seconds before next document...\n",
      "\n",
      "Processing document 2/4: 2\n",
      "  Waiting 30 seconds before text embedding...\n",
      "  Generating text embedding...\n",
      "  ✓ Text embedding generated\n",
      "  Waiting 45 seconds before image processing...\n",
      "  Encoding image...\n",
      "  Generating image embedding...\n",
      "  ✓ Image embedding generated\n",
      "  ✓ Document 2 processed successfully\n",
      "  Waiting 60 seconds before next document...\n",
      "\n",
      "Processing document 3/4: 3\n",
      "  Waiting 30 seconds before text embedding...\n",
      "  Generating text embedding...\n",
      "  ✓ Text embedding generated\n",
      "  Waiting 45 seconds before image processing...\n",
      "  Encoding image...\n",
      "  Generating image embedding...\n",
      "  ✓ Image embedding generated\n",
      "  ✓ Document 3 processed successfully\n",
      "  Waiting 60 seconds before next document...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# APPROACH: Manual embedding generation with rate limit handling\n",
    "# We're generating embeddings manually to handle the strict rate limits of the Cohere Embed-4 API.\n",
    "# This approach downloads images, converts them to base64, and calls the Embed-4 API directly.\n",
    "# We add significant delays between API calls due to strict rate limits on the endpoint.\n",
    "# \n",
    "# NOTE: Once rate limits are resolved (higher quota/tier or less busy endpoint), you could:\n",
    "# - Remove the time.sleep() delays for faster processing\n",
    "# - Use Azure's built-in vectorizers once Embed-4 is added to the model catalog\n",
    "# - Process documents in parallel/batches for much better performance\n",
    "\n",
    "processed_documents = []\n",
    "\n",
    "print(\"Processing documents and generating embeddings...\")\n",
    "print(\"Using extended delays to handle API rate limits - this will take several minutes.\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "   print(f\"\\nProcessing document {i+1}/{len(documents)}: {doc['id']}\")\n",
    "   \n",
    "   caption = doc[\"caption\"]\n",
    "   image_url = doc[\"imageUrl\"]\n",
    "   \n",
    "   try:\n",
    "       # Generate text embedding from caption\n",
    "       print(\"  Waiting 30 seconds to avoid rate limits...\")\n",
    "       time.sleep(30)\n",
    "       \n",
    "       print(\"  Generating text embedding...\")\n",
    "       caption_embedding = generate_text_embedding(caption)\n",
    "       print(\"  ✓ Text embedding generated\")\n",
    "       \n",
    "       # Generate image embedding from URL\n",
    "       print(\"  Waiting 45 seconds before image processing...\")\n",
    "       time.sleep(45)\n",
    "       \n",
    "       print(\"  Downloading and encoding image...\")\n",
    "       image_base64 = encode_image_to_base64(image_url)\n",
    "       \n",
    "       if image_base64:\n",
    "           print(\"  Generating image embedding...\")\n",
    "           image_embedding = generate_image_embedding(image_base64)\n",
    "           print(\"  ✓ Image embedding generated\")\n",
    "       else:\n",
    "           image_embedding = None\n",
    "           print(\"  ✗ Failed to encode image\")\n",
    "       \n",
    "       # Create document with embeddings for indexing\n",
    "       doc_copy = doc.copy()\n",
    "       doc_copy[\"captionVector\"] = caption_embedding\n",
    "       doc_copy[\"imageVector\"] = image_embedding\n",
    "       \n",
    "       processed_documents.append(doc_copy)\n",
    "       print(f\"  ✓ Document {doc['id']} processed successfully\")\n",
    "       \n",
    "       # Wait between documents to respect rate limits\n",
    "       if i < len(documents) - 1:\n",
    "           print(\"  Waiting 60 seconds before next document...\")\n",
    "           time.sleep(60)\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"  ✗ Error processing document {doc['id']}: {e}\")\n",
    "       # Add document without embeddings to maintain data integrity\n",
    "       doc_copy = doc.copy()\n",
    "       doc_copy[\"captionVector\"] = None\n",
    "       doc_copy[\"imageVector\"] = None\n",
    "       processed_documents.append(doc_copy)\n",
    "       \n",
    "       if i < len(documents) - 1:\n",
    "           print(\"  Waiting 60 seconds after error...\")\n",
    "           time.sleep(60)\n",
    "\n",
    "print(f\"\\nProcessing complete! {len(processed_documents)} documents with embeddings ready for indexing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149cb05",
   "metadata": {},
   "source": [
    "## Create an Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields for the index\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(\n",
    "        name=\"imageUrl\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        retrievable=True,\n",
    "        filterable=True\n",
    "    ),\n",
    "    SimpleField(\n",
    "        name=\"caption\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "        retrievable=True\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"imageVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=EMBEDDING_DIMENSIONS,  # Embed-4 uses 1536 dimensions\n",
    "        vector_search_profile_name=\"vector_profile\"\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"captionVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=EMBEDDING_DIMENSIONS,  # Embed-4 uses 1536 dimensions\n",
    "        vector_search_profile_name=\"vector_profile\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector search configuration\n",
    "# Using manual embedding generation for Embed-4 compatibility\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"hnsw_config\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"vector_profile\", \n",
    "            algorithm_configuration_name=\"hnsw_config\",\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd24f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index multimodal-cohere-embed4-index created successfully\n",
      "\n",
      "Index fields:\n",
      "- id (Edm.String)\n",
      "- imageUrl (Edm.String)\n",
      "- caption (Edm.String)\n",
      "- imageVector (Collection(Edm.Single))\n",
      "- captionVector (Collection(Edm.Single))\n"
     ]
    }
   ],
   "source": [
    "# Create the index\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "try:\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"Index {INDEX_NAME} created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {str(e)}\")\n",
    "\n",
    "# Optional: Verify index fields\n",
    "try:\n",
    "    index_info = index_client.get_index(INDEX_NAME)\n",
    "    print(\"\\nIndex fields:\")\n",
    "    for field in index_info.fields:\n",
    "        print(f\"- {field.name} ({field.type})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving index fields: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8876a",
   "metadata": {},
   "source": [
    "## Upload Documents to the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc29ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = search_client.merge_or_upload_documents(documents=processed_documents)\n",
    "    if result[0].succeeded:\n",
    "        print(\"Documents uploaded successfully.\")\n",
    "    else:\n",
    "        print(\"Error uploading documents.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169789a",
   "metadata": {},
   "source": [
    "## Perform Multimodal Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of each search function\n",
    "query_text = \"sports activities like skateboarding\"\n",
    "image_url = \"https://portal.vision.cognitive.azure.com/dist/assets/ImageCaptioningSample5-14b26724.png\"  ## Image of man on skateboard\n",
    "\n",
    "# Helper function to display results\n",
    "def display_results(results):\n",
    "    for result in results:\n",
    "        print(f\"Caption: {result['caption']}\")\n",
    "        print(f\"Score: {result['@search.score']}\")\n",
    "        print(f\"URL: {result['imageUrl']}\")\n",
    "        display(HTML(f'<img src=\"{result[\"imageUrl\"]}\" style=\"width:200px;\"/>'))\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881154bd",
   "metadata": {},
   "source": [
    "### Text to Text Vector Search\n",
    "\n",
    "This section performs a vector search to find similar text entries based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Text Vector Search Results:\n",
      "Caption: A man photographs two girls sitting in an open car trunk, capturing a casual outdoor photography session with friends\n",
      "Score: 0.5598682\n",
      "URL: https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Text to Text Vector Search\n",
    "def text_to_text_search(query_text):\n",
    "    # Generate text embedding\n",
    "    text_embedding = generate_text_embedding(query_text)\n",
    "    \n",
    "    # Define the text vector query\n",
    "    text_vector_query = VectorizedQuery(\n",
    "        vector=text_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"captionVector\"\n",
    "    )\n",
    "    \n",
    "    # Perform search\n",
    "    results = search_client.search(\n",
    "        search_text=None, vector_queries=[text_vector_query], top=1\n",
    "    )\n",
    "    display_results(results)\n",
    "\n",
    "# Example usage\n",
    "print(\"Text to Text Vector Search Results:\")\n",
    "text_to_text_search(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6f99d",
   "metadata": {},
   "source": [
    "### Text to Image Vector Search\n",
    "\n",
    "This section performs a vector search to find images related to the query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710893fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Image Vector Search Results:\n",
      "Caption: A hockey player takes aim for the goal while facing the goalie on an ice rink, capturing the intensity and precision of competitive ice hockey\n",
      "Score: 0.554676\n",
      "URL: https://images.unsplash.com/photo-1516226415502-d6624544376b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1516226415502-d6624544376b\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Text to Image Vector Search\n",
    "def text_to_image_search(query_text):\n",
    "    # Generate text embedding\n",
    "    text_embedding = generate_text_embedding(query_text)\n",
    "    \n",
    "    # Define the text-to-image vector query\n",
    "    text_to_image_query = VectorizedQuery(\n",
    "        vector=text_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"imageVector\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Perform search\n",
    "    results = search_client.search(\n",
    "        search_text=None, vector_queries=[text_to_image_query], top=1\n",
    "    )\n",
    "    display_results(results)\n",
    "\n",
    "# Example usage\n",
    "print(\"Text to Image Vector Search Results:\")\n",
    "text_to_image_search(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f203b3b",
   "metadata": {},
   "source": [
    "### Image to Text Vector Search\n",
    "\n",
    "This section performs a vector search to find text entries related to a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image to Text Vector Search Results:\n",
      "Caption: A man wearing a beanie and shirt works on a laptop, representing modern remote work and technology usage\n",
      "Score: 0.5489908\n",
      "URL: https://images.unsplash.com/photo-1755541516517-bb95790dc7ad\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1755541516517-bb95790dc7ad\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Image to Text Vector Search\n",
    "def image_to_text_search(image_url):\n",
    "    # Generate image embedding\n",
    "    image_base64 = encode_image_to_base64(image_url)\n",
    "    image_embedding = generate_image_embedding(image_base64)\n",
    "    \n",
    "    # Define the image-to-text vector query\n",
    "    image_to_text_query = VectorizedQuery(\n",
    "        vector=image_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"captionVector\"\n",
    "    )\n",
    "    \n",
    "    # Perform search\n",
    "    results = search_client.search(\n",
    "        search_text=None, vector_queries=[image_to_text_query], top=1\n",
    "    )\n",
    "    display_results(results)\n",
    "\n",
    "# Example usage\n",
    "print(\"Image to Text Vector Search Results:\")\n",
    "image_to_text_search(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c6238",
   "metadata": {},
   "source": [
    "## Cross-Field Vector Search: Text Embedding Query\n",
    "\n",
    "This section performs a cross-field vector search using a text embedding to query both image and caption fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a81cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Field Vector Search Results (Text Embedding Query):\n",
      "Caption: A man photographs two girls sitting in an open car trunk, capturing a casual outdoor photography session with friends\n",
      "Score: 0.01666666753590107\n",
      "URL: https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Caption: A hockey player takes aim for the goal while facing the goalie on an ice rink, capturing the intensity and precision of competitive ice hockey\n",
      "Score: 0.01666666753590107\n",
      "URL: https://images.unsplash.com/photo-1516226415502-d6624544376b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1516226415502-d6624544376b\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cross-Field Vector Search: Text Embedding Query\n",
    "def text_embedding_cross_field_search(query_text, return_results=False):\n",
    "    # Generate text embedding\n",
    "    text_embedding = generate_text_embedding(query_text)\n",
    "    \n",
    "    # Define the vector query for both caption and image fields\n",
    "    cross_field_query = VectorizedQuery(\n",
    "        vector=text_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"imageVector, captionVector\"\n",
    "    )\n",
    "    \n",
    "    # Perform search\n",
    "    results = search_client.search(\n",
    "        search_text=None, vector_queries=[cross_field_query], top=3\n",
    "    )\n",
    "    \n",
    "    if return_results:\n",
    "        return results\n",
    "    else:\n",
    "        display_results(results)\n",
    "\n",
    "# Example usage\n",
    "print(\"Cross-Field Vector Search Results (Text Embedding Query):\")\n",
    "text_embedding_cross_field_search(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffced40",
   "metadata": {},
   "source": [
    "## Multi-Vector Search: Text and Image Query\n",
    "\n",
    "This section performs a multi-vector search using both text and image vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text and Image Multi-Vector Query Results:\n",
      "Caption: A man wearing a beanie and shirt works on a laptop, representing modern remote work and technology usage\n",
      "Score: 0.01666666753590107\n",
      "URL: https://images.unsplash.com/photo-1755541516517-bb95790dc7ad\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1755541516517-bb95790dc7ad\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Caption: A man photographs two girls sitting in an open car trunk, capturing a casual outdoor photography session with friends\n",
      "Score: 0.01666666753590107\n",
      "URL: https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images.unsplash.com/photo-1756142007155-c8b4eb0c3808\" style=\"width:200px;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multi-Vector Search: Text and Image Query\n",
    "def text_and_image_query_multi_vector(query_text, image_url):\n",
    "    # Generate the text embedding\n",
    "    text_embedding = generate_text_embedding(query_text)\n",
    "    \n",
    "    # Encode the image and generate the image embedding\n",
    "    image_base64 = encode_image_to_base64(image_url)\n",
    "    image_embedding = generate_image_embedding(image_base64)\n",
    "\n",
    "    # Define the text vector query\n",
    "    text_vector_query = VectorizedQuery(\n",
    "        vector=text_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"captionVector\"\n",
    "    )\n",
    "    \n",
    "    # Define the image vector query\n",
    "    image_vector_query = VectorizedQuery(\n",
    "        vector=image_embedding,\n",
    "        k_nearest_neighbors=1,\n",
    "        fields=\"imageVector\"\n",
    "    )\n",
    "\n",
    "    # Perform the search with both vector queries\n",
    "    results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector_queries=[text_vector_query, image_vector_query],\n",
    "        top=2\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    display_results(results)\n",
    "\n",
    "# Example usage\n",
    "print(\"Text and Image Multi-Vector Query Results:\")\n",
    "text_and_image_query_multi_vector(query_text, image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c8002",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete workflow for using Cohere Embed-4 with Azure AI Search for multimodal vector search applications. Key capabilities showcased include:\n",
    "\n",
    "1. **Direct REST API Integration**: Updated to use direct REST API calls to Cohere Embed-4 endpoints instead of the SDK\n",
    "2. **Multimodal Embeddings**: Generate embeddings for text and image content using the latest Embed-4 model\n",
    "3. **Advanced Vector Search**: Implement various search patterns including cross-field and multi-vector queries\n",
    "4. **Visual Results**: Display images inline for immediate visual feedback\n",
    "\n",
    "The Embed-4 model provides enhanced multimodal capabilities with 1536-dimensional embeddings, offering improved performance over previous versions for enterprise AI applications including image search and cross-modal retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
